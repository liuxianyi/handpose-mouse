{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyautogui as mouse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandTracker():\n",
    "    r\"\"\"\n",
    "    Class to use Google's Mediapipe HandTracking pipeline from Python.\n",
    "    So far only detection of a single hand is supported.\n",
    "    Any any image size and aspect ratio supported.\n",
    "    Args:\n",
    "        palm_model: path to the palm_detection.tflite\n",
    "        joint_model: path to the hand_landmark.tflite\n",
    "        anchors_path: path to the csv containing SSD anchors\n",
    "    Ourput:\n",
    "        (21,2) array of hand joints.\n",
    "    Examples::\n",
    "        # >>> det = HandTracker(path1, path2, path3)\n",
    "        # >>> input_img = np.random.randint(0,255, 256*256*3).reshape(256,256,3)\n",
    "        # >>> keypoints, bbox = det(input_img)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, palm_model, joint_model, anchors_path,\n",
    "                 box_enlarge=1.5, box_shift=0.2):\n",
    "        self.box_shift = box_shift\n",
    "        self.box_enlarge = box_enlarge\n",
    "\n",
    "        self.interp_palm = tf.lite.Interpreter(palm_model)\n",
    "        self.interp_palm.allocate_tensors()\n",
    "        self.interp_joint = tf.lite.Interpreter(joint_model)\n",
    "        self.interp_joint.allocate_tensors()\n",
    "\n",
    "        # reading the SSD anchors\n",
    "        with open(anchors_path, \"r\") as csv_f:\n",
    "            self.anchors = np.r_[\n",
    "                [x for x in csv.reader(csv_f, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "            ]\n",
    "        # reading tflite model paramteres\n",
    "        output_details = self.interp_palm.get_output_details()\n",
    "        input_details = self.interp_palm.get_input_details()\n",
    "\n",
    "        self.in_idx = input_details[0]['index']\n",
    "        self.out_reg_idx = output_details[0]['index']\n",
    "        self.out_clf_idx = output_details[1]['index']\n",
    "\n",
    "        self.in_idx_joint = self.interp_joint.get_input_details()[0]['index']\n",
    "        self.out_idx_joint = self.interp_joint.get_output_details()[0]['index']\n",
    "\n",
    "        # 90° rotation matrix used to create the alignment trianlge\n",
    "        self.R90 = np.r_[[[0, 1], [-1, 0]]]\n",
    "\n",
    "        # trianlge target coordinates used to move the detected hand\n",
    "        # into the right position\n",
    "        self._target_triangle = np.float32([\n",
    "            [128, 128],\n",
    "            [128, 0],\n",
    "            [0, 128]\n",
    "        ])\n",
    "        self._target_box = np.float32([\n",
    "            [0, 0, 1],\n",
    "            [256, 0, 1],\n",
    "            [256, 256, 1],\n",
    "            [0, 256, 1],\n",
    "        ])\n",
    "\n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "        \"\"\"get a triangle used to calculate Affine transformation matrix\"\"\"\n",
    "\n",
    "        dir_v = kp2 - kp0\n",
    "        dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "        dir_v_r = dir_v @ self.R90.T\n",
    "        return np.float32([kp2, kp2 + dir_v * dist, kp2 + dir_v_r * dist])\n",
    "\n",
    "    @staticmethod\n",
    "    def _triangle_to_bbox(source):\n",
    "        # plain old vector arithmetics\n",
    "        bbox = np.c_[\n",
    "            [source[2] - source[0] + source[1]],\n",
    "            [source[1] + source[0] - source[2]],\n",
    "            [3 * source[0] - source[1] - source[2]],\n",
    "            [source[2] - source[1] + source[0]],\n",
    "        ].reshape(-1, 2)\n",
    "        return bbox\n",
    "\n",
    "    @staticmethod\n",
    "    def _im_normalize(img):\n",
    "        return np.ascontiguousarray(\n",
    "            2 * ((img / 255) - 0.5\n",
    "                 ).astype('float32'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigm(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad1(x):\n",
    "        return np.pad(x, ((0, 0), (0, 1)), constant_values=1, mode='constant')\n",
    "\n",
    "    def predict_joints(self, img_norm):\n",
    "        self.interp_joint.set_tensor(\n",
    "            self.in_idx_joint, img_norm.reshape(1, 256, 256, 3))\n",
    "        self.interp_joint.invoke()\n",
    "\n",
    "        joints = self.interp_joint.get_tensor(self.out_idx_joint)\n",
    "        return joints.reshape(-1, 2)\n",
    "\n",
    "    def detect_hand(self, img_norm):\n",
    "        assert -1 <= img_norm.min() and img_norm.max() <= 1, \\\n",
    "            \"img_norm should be in range [-1, 1]\"\n",
    "        assert img_norm.shape == (256, 256, 3), \\\n",
    "            \"img_norm shape must be (256, 256, 3)\"\n",
    "\n",
    "        # predict hand location and 7 initial landmarks\n",
    "        self.interp_palm.set_tensor(self.in_idx, img_norm[None])\n",
    "        self.interp_palm.invoke()\n",
    "\n",
    "        out_reg = self.interp_palm.get_tensor(self.out_reg_idx)[0]\n",
    "        out_clf = self.interp_palm.get_tensor(self.out_clf_idx)[0, :, 0]\n",
    "\n",
    "        # finding the best prediction\n",
    "        # TODO: replace it with non-max suppression\n",
    "        detecion_mask = self._sigm(out_clf) > 0.7\n",
    "        candidate_detect = out_reg[detecion_mask]\n",
    "        candidate_anchors = self.anchors[detecion_mask]\n",
    "\n",
    "        if candidate_detect.shape[0] == 0:\n",
    "            # print(\"No hands found\")\n",
    "            return None, None\n",
    "        # picking the widest suggestion while NMS is not implemented\n",
    "        max_idx = np.argmax(candidate_detect[:, 3])\n",
    "\n",
    "        # bounding box offsets, width and height\n",
    "        dx, dy, w, h = candidate_detect[max_idx, :4]\n",
    "        center_wo_offst = candidate_anchors[max_idx, :2] * 256\n",
    "\n",
    "        # 7 initial keypoints\n",
    "        keypoints = center_wo_offst + candidate_detect[max_idx, 4:].reshape(-1, 2)\n",
    "        side = max(w, h) * self.box_enlarge\n",
    "\n",
    "        # now we need to move and rotate the detected hand for it to occupy a\n",
    "        # 256x256 square\n",
    "        # line from wrist keypoint to middle finger keypoint\n",
    "        # should point straight up\n",
    "        # TODO: replace triangle with the bbox directly\n",
    "        source = self._get_triangle(keypoints[0], keypoints[2], side)\n",
    "        source -= (keypoints[0] - keypoints[2]) * self.box_shift\n",
    "        return source, keypoints\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        # fit the image into a 256x256 square\n",
    "        shape = np.r_[img.shape]\n",
    "        pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "        img_pad = np.pad(\n",
    "            img,\n",
    "            ((pad[0], pad[0]), (pad[1], pad[1]), (0, 0)),\n",
    "            mode='constant')\n",
    "        img_small = cv2.resize(img_pad, (256, 256))\n",
    "        img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "        img_norm = self._im_normalize(img_small)\n",
    "        return img_pad, img_norm, pad\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_pad, img_norm, pad = self.preprocess_img(img)\n",
    "\n",
    "        source, keypoints = self.detect_hand(img_norm)\n",
    "        if source is None:\n",
    "            return None, None\n",
    "\n",
    "        # calculating transformation from img_pad coords\n",
    "        # to img_landmark coords (cropped hand image)\n",
    "        scale = max(img.shape) / 256\n",
    "        Mtr = cv2.getAffineTransform(\n",
    "            source * scale,\n",
    "            self._target_triangle\n",
    "        )\n",
    "\n",
    "        img_landmark = cv2.warpAffine(\n",
    "            self._im_normalize(img_pad), Mtr, (256, 256)\n",
    "        )\n",
    "\n",
    "        joints = self.predict_joints(img_landmark)\n",
    "\n",
    "        # adding the [0,0,1] row to make the matrix square\n",
    "        Mtr = self._pad1(Mtr.T).T\n",
    "        Mtr[2, :2] = 0\n",
    "\n",
    "        Minv = np.linalg.inv(Mtr)\n",
    "\n",
    "        # projecting keypoints back into original image coordinate space\n",
    "        kp_orig = (self._pad1(joints) @ Minv.T)[:, :2]\n",
    "        box_orig = (self._target_box @ Minv.T)[:, :2]\n",
    "        kp_orig -= pad[::-1]\n",
    "        box_orig -= pad[::-1]\n",
    "\n",
    "        return kp_orig, box_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base = os.path.join(os.path.abspath(\"\"), \"tensorflow_test/handTracking/\")\n",
    "palm_model_path = os.path.join(base, \"palm_detection_without_custom_op.tflite\")\n",
    "landmark_model_path = os.path.join(base, \"hand_landmark.tflite\")\n",
    "anchors_path = os.path.join(base, \"anchors.csv\")\n",
    "test_img =  os.path.join(base, \"test_img.jpg\")\n",
    "\n",
    "assert os.path.isfile(palm_model_path) \n",
    "assert os.path.isfile(landmark_model_path) \n",
    "assert os.path.isfile(anchors_path) \n",
    "assert os.path.isfile(test_img)\n",
    "img = cv2.imread(test_img)[:, :, ::-1]\n",
    "detector = HandTracker(palm_model_path, landmark_model_path, anchors_path,\n",
    "                   box_shift=0.2, box_enlarge=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 3072, 3)\n(614, 819, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "ig = cv2.resize(img, (int(img.shape[0]/5), int(img.shape[1]/5)))\n",
    "print(ig.shape)\n",
    "cv2.imshow(\"goog\", ig)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goog\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:89: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed 0.389896 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "ig = cv2.resize(img, (1000, 1000))\n",
    "kp, box = detector(ig)\n",
    "# print(kp)\n",
    "# cv2.namedWindow(\"goog\")\n",
    "# cv2.resizeWindow(\"goog\", 10, 10)\n",
    "center = []\n",
    "for i in range(4):\n",
    "    center.append((kp[5+i*4]+kp[0])/2)\n",
    "centerPoint = np.sum(center, axis=0)/4\n",
    "# print(centerPoint)\n",
    "cv2.circle(ig, (int(centerPoint[0]), int(centerPoint[1])), radius=10, \n",
    "               color=(255, 255, 255), thickness=-1,\n",
    "               lineType=cv2.FILLED)\n",
    "# for i in center:\n",
    "#     cv2.circle(ig, (int(i[0]), int(i[1])), radius=4, \n",
    "#                color=(0, 255, 255), thickness=-1,\n",
    "#                lineType=cv2.FILLED)\n",
    "for i in kp:\n",
    "    cv2.circle(ig, (int(i[0]), int(i[1])), radius=4, \n",
    "               color=(0, 0, 255), thickness=-1,\n",
    "               lineType=cv2.FILLED)\n",
    "cv2.imshow(\"goog\", ig)\n",
    "#print(box)\n",
    "# f,ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "# \n",
    "# ax.imshow(img)\n",
    "# \n",
    "# ax.scatter(kp[:, 0], kp[:, 1])\n",
    "# ax.add_patch(Polygon(box, color=\"#00ff00\", fill=False))\n",
    "end = time.time()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"speed %.6f \" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goog\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:89: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mouse.FAILSAFE = False\n",
    "arr_x = np.array([0,0,0], dtype=float)\n",
    "arr_y = np.array([0,0,0], dtype=float)\n",
    "avg_x = 0.0\n",
    "avg_y = 0.0\n",
    "x = 0.0\n",
    "y = 0.0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    # frame = frame[:, ::-1, :]\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    kp, box = detector(frame)\n",
    "    # print(frame.shape)\n",
    "    # break\n",
    "    if kp is not None:\n",
    "        # 找到手掌心\n",
    "        start = time.time()\n",
    "        center = []\n",
    "        for i in range(4): \n",
    "            center.append((kp[5+i*4]+kp[0])/2)\n",
    "        centerPoint_x, centerPoint_y = np.sum(center, axis=0)/4\n",
    "        # 0: 距离左边的距离\n",
    "        # 1：距离上边的距离\n",
    "        # img_size = [640, 480]\n",
    "        # print(centerPoint)\n",
    "        # 左右上下裁剪100\n",
    "        # 映射范围裁剪\n",
    "        \n",
    "        cv2.circle(frame, (int(centerPoint_x), int(centerPoint_y)), radius=10, \n",
    "                       color=(255, 255, 255), thickness=-1,\n",
    "                       lineType=cv2.FILLED)\n",
    "        \n",
    "        # 映射    \n",
    "        x = 1920*(centerPoint_x-120)/400\n",
    "        y = 1080*(centerPoint_y-120)/240\n",
    "        if x<0:\n",
    "            x = 0\n",
    "        if y<0:\n",
    "            y=0\n",
    "        if x>1920:\n",
    "            x=1920\n",
    "        if y>1080:\n",
    "            y=1080\n",
    "        arr_y[2] = arr_y[1]\n",
    "        arr_x[2] = arr_x[1]\n",
    "        arr_y[1] = arr_y[0]\n",
    "        arr_x[1] = arr_x[0]\n",
    "        arr_x[0] = x\n",
    "        arr_y[0] = y\n",
    "        \n",
    "        avg_x = np.mean(arr_x)\n",
    "        avg_y = np.mean(arr_y)\n",
    "        mouse.moveTo(int(avg_x), int(avg_y))\n",
    "        # print(centerPoint)\n",
    "        \n",
    "        \n",
    "        for i in kp:\n",
    "            cv2.circle(frame, (int(i[0]), int(i[1])), radius=10, \n",
    "                       color=(0, 255, 0),\n",
    "                       thickness=-1, lineType=cv2.FILLED)\n",
    "    \n",
    "    cv2.imshow(\"goog\", frame)\n",
    "    \n",
    "    end = time.time()\n",
    "    # print(\"speed %.6f \" % (end-start))\n",
    "    if cv2.waitKey(20)&0xff == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "time.sleep(1)\n",
    "mouse.dragRel(100, 10, 1)\n",
    "mouse.doubleClick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
